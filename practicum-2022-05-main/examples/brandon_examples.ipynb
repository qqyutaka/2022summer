{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a97b52b6-ee43-49b0-a303-34af4c4516f9",
   "metadata": {},
   "source": [
    "# Examples of using some of Brandon's signal processing algorithms\n",
    "\n",
    "This notebook contains examples showing how to use some of Brandon's algorithms.  This includes:\n",
    "- **Signal strength feature computation** -- These are like a spectrogram, but drastically reduce the effects of background noise and differences microphone gain / sensitivity.  (Note: The name \"signal strength features\" is just a name Brandon made up to refer to them.)\n",
    "- **Auto-segmentation** -- This tries to locate the beginning and ending of all the sounds in a recording that stick out relative to the background noise.\n",
    "- **Pitch tracking** -- This tries to track a dominant pitch across the length of a segment.  Primarily intended for use with chirp sounds.\n",
    "- Utilities and methods to use the above to extract features for segments, reduce their dimensionality, cluster sound types, etc.\n",
    "\n",
    "Note that while it may appear that there is a lot of code in this notebook, most of it is plotting code.  Generally, the first few lines of each code cell contain all the relevant code, and the remainder of the code in the cell (below a comment saying `Visualization code below`) is just to create and format plots to visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b4e98-e344-41f5-9827-932435edfda1",
   "metadata": {},
   "source": [
    "# If running in SageMaker, set up Python environment / install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221905a-b717-47d7-be6a-8a94d1ebedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib.util\n",
    "\n",
    "def is_package_installed(package_name):\n",
    "    if package_name in sys.modules:\n",
    "        return True\n",
    "    else:\n",
    "        return importlib.util.find_spec(package_name) is not None\n",
    "\n",
    "running_on_aws = True if os.environ.get(\"AWS_DEFAULT_REGION\") else False\n",
    "if not running_on_aws:\n",
    "    print(\"Not running on AWS -- Assume that the Python environment is already set up.\")\n",
    "else:\n",
    "    print(\"Running on AWS -- Making sure needed packages are installed...\")\n",
    "    if is_package_installed(\"soundfile\"):\n",
    "        print(\"  Soundfile already installed.\")\n",
    "    else:\n",
    "        print(\"\\n\\nInstalling soundfile...\")\n",
    "        #%pip install pysoundfile\n",
    "        %conda install -c conda-forge pysoundfile\n",
    "    if is_package_installed(\"librosa\"):\n",
    "        print(\"  Librosa already installed.\")\n",
    "    else:\n",
    "        print(\"\\n\\nInstalling librosa (this may take a while)...\")\n",
    "        #%pip install librosa\n",
    "        %conda install -c conda-forge librosa\n",
    "    if is_package_installed(\"audiot\"):\n",
    "        print(\"  AudioT package already installed.\")\n",
    "    else:\n",
    "        print(\"\\n\\nInstalling AudioT package in development / editable mode...\")\n",
    "        # Use the relative path to the folder containing setup.py, which is the parent folder ../ in this case\n",
    "        %pip install -e ../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a29e83-bacb-4a22-9c25-130d2892d8c3",
   "metadata": {},
   "source": [
    "### <font color='red'>**If any new packages were installed above, restart the kernel before running the remainder of this notebook**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b09fb8-c710-464e-8365-81a14af634b9",
   "metadata": {},
   "source": [
    "# Imports / Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7aa26f-efe3-4861-80d1-b13dd4dcaa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random\n",
    "import scipy as sp\n",
    "from pathlib import Path\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from audiot.audio_signal import AudioSignal\n",
    "from audiot.spectral_features import resize_matrix_by_averaging\n",
    "from audiot.signal_processing.functions import calc_signal_strength_features, compute_pitch_upsweep_downsweep, compute_chirp_features_for_segments\n",
    "from audiot.signal_processing.auto_segmenter import AutoSegmenter\n",
    "from audiot.signal_processing.pitch_tracker import PitchTracker\n",
    "\n",
    "# Show matplotlib plots inline in Jupyter notebooks without having to call show()\n",
    "%matplotlib inline\n",
    "\n",
    "# Import for audio player control\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f45bca-6eb4-494c-ae49-1520e0b2ff63",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "Set parameters for the size of the figures (adjust this if needed for your monitor / resolution), the segment of the signal we'll be zooming in on, and for spectrogram computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c60444-0722-4cbf-8c92-be18ad0705e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the figure size as desired so that the plots fit your monitor / screen resolution\n",
    "figure_size = (30, 4)\n",
    "small_figure_size = (figure_size[0]/3, figure_size[1])\n",
    "\n",
    "# Construct the path to the test file\n",
    "project_folder = Path(\"\").absolute().parents[0]\n",
    "file_path = project_folder / \"test_data\" / \"TRF0_mic14_2020-12-17_01.20.00.flac\"\n",
    "\n",
    "# Define the start and end time for the segment we'll be zooming in on\n",
    "segment_of_interest_start_time = 5\n",
    "segment_of_interest_end_time = 10\n",
    "fig_xlim = (segment_of_interest_start_time, segment_of_interest_end_time)\n",
    "\n",
    "# Parameters for short time fourier transforms (STFTs) -- for spectrogram plots\n",
    "fft_n_samples_per_window = 256\n",
    "fft_n_overlap = fft_n_samples_per_window / 2\n",
    "fft_nfft = 2 ** np.ceil(np.log2(fft_n_samples_per_window))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ea992-40f1-43eb-b9fb-9d8e35f5beca",
   "metadata": {},
   "source": [
    "# Signal strength features\n",
    "\n",
    "These features represent an estimate of how much energy in each cell of the spectrogram is from a foreground sound versus from a background sound.  The values will always fall in the range [0.0, 1.0], where a value of 0.0 indicates all background noise and a value of 1.0 indicates all foreground noise with zero background noise.  The estimate is weighted towards considering most things as background noise, so only sounds that stick out significantly will show up while most everything else gets zeroed out.  A value of 0.3 would indicate that 30% of the energy in that spot in the spectrogram was estimated to be from a foreground sound, with the remaining 70% of the energy coming from background noise.\n",
    "\n",
    "Note that the features are returned as a `SpectralFeatures` object instead of as an `AudioFeatures` object.  These are similar classes that I'll probably want to merge (or inherit one from the other), but I didn't want to remove or change the `AudioFeatures` class in the middle of a practicum cohort because it might break people's code.  The `SpectralFeatures` objects can be converted to `AudioFeatures` via the `SpectralFeatures.as_audio_features()` method.\n",
    "\n",
    "Below, we compute and plot both a spectrogram of the signal and the signal strength features for comparison.  An audio control is also embedded in the output that you can use to listen to the marked segment of the recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb777c4-0c5a-475c-8834-0bfd474e9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the audio file\n",
    "audio_signal = AudioSignal.from_file(file_path)\n",
    "# Compute the signal strength features\n",
    "signal_strength_features = calc_signal_strength_features(audio_signal)\n",
    "\n",
    "# --- Visualization code below ---------------------------------------------------------------------\n",
    "\n",
    "# Compute a spectrogram for the audio recording (for visual comparison)\n",
    "(spectrogram_frequencies, spectrogram_times, spectrogram_magnitude) = sp.signal.spectrogram(\n",
    "    audio_signal.signal[:, 0],\n",
    "    fs=audio_signal.sample_rate,\n",
    "    nperseg=fft_n_samples_per_window,\n",
    "    noverlap=fft_n_overlap,\n",
    "    nfft=fft_nfft,\n",
    ")\n",
    "spectrogram = np.log(spectrogram_magnitude)\n",
    "\n",
    "# Plot the spectrogram\n",
    "fig = plt.figure(figsize=figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    spectrogram,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    extent=[\n",
    "        spectrogram_times[0],\n",
    "        spectrogram_times[-1],\n",
    "        spectrogram_frequencies[0],\n",
    "        spectrogram_frequencies[-1],\n",
    "    ],\n",
    ")\n",
    "# Mark the segment we'll be zooming in on with vertical, red lines\n",
    "axes.vlines([segment_of_interest_start_time, segment_of_interest_end_time], 0, 8000, color=\"r\")\n",
    "axes.set_xlabel(\"Time (seconds)\")\n",
    "axes.set_ylabel(\"Frequency (Hz)\")\n",
    "axes.set_title(\"Spectrogram\")\n",
    "\n",
    "# Plot the signal strength features\n",
    "fig = plt.figure(figsize=figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    signal_strength_features.features,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    extent=[\n",
    "        signal_strength_features.time_axis[0],\n",
    "        signal_strength_features.time_axis[-1],\n",
    "        signal_strength_features.frequency_axis[0],\n",
    "        signal_strength_features.frequency_axis[-1],\n",
    "    ],\n",
    ")\n",
    "# Mark the segment we'll be zooming in on with vertical, red lines\n",
    "axes.vlines([segment_of_interest_start_time, segment_of_interest_end_time], 0, 8000, color=\"r\")\n",
    "axes.set_xlabel(\"Time (seconds)\")\n",
    "axes.set_ylabel(\"Frequency (Hz)\")\n",
    "axes.set_title(\"Signal strength features\")\n",
    "\n",
    "# Add a control to listen to the segment we zoomed in on\n",
    "print(\"Play the marked segment:\")\n",
    "ipd.Audio(\n",
    "    audio_signal.signal[\n",
    "        int(segment_of_interest_start_time * audio_signal.sample_rate) : int(\n",
    "            segment_of_interest_end_time * audio_signal.sample_rate\n",
    "        ),\n",
    "        -1,\n",
    "    ],\n",
    "    rate=int(audio_signal.sample_rate),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ceb6aa-f3e5-4f71-b63e-5a03470ec525",
   "metadata": {},
   "source": [
    "### Zoom in for better view\n",
    "\n",
    "Here, we make the same plots as above, but adjust the x-axis limits to zoom in on the marked segment so it's easier to see detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467396d-fe99-4ab8-b67b-cee95de8eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization code below ---------------------------------------------------------------------\n",
    "\n",
    "# Plot the spectrogram zoomed in on the segment\n",
    "fig = plt.figure(figsize=figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    spectrogram,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        spectrogram_times[0],\n",
    "        spectrogram_times[-1],\n",
    "        spectrogram_frequencies[0],\n",
    "        spectrogram_frequencies[-1],\n",
    "    ],\n",
    ")\n",
    "axes.set_xlabel(\"Time (seconds)\")\n",
    "axes.set_ylabel(\"Frequency (Hz)\")\n",
    "axes.set_xlim(fig_xlim)\n",
    "\n",
    "# Plot the signal strength features zoomed in on the segment\n",
    "fig = plt.figure(figsize=figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    signal_strength_features.features,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        signal_strength_features.time_axis[0],\n",
    "        signal_strength_features.time_axis[-1],\n",
    "        signal_strength_features.frequency_axis[0],\n",
    "        signal_strength_features.frequency_axis[-1],\n",
    "    ],\n",
    ")\n",
    "axes.set_xlabel(\"Time (seconds)\")\n",
    "axes.set_ylabel(\"Frequency (Hz)\")\n",
    "axes.set_xlim(fig_xlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f74bc-08b1-4647-b4ae-cc003a9beaea",
   "metadata": {},
   "source": [
    "# Auto-segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fc9602-0307-4b1e-8f2d-8b76597ceaea",
   "metadata": {},
   "source": [
    "## Segmenting an AudioSignal directly\n",
    "\n",
    "If you just want to segment a signal and don't need the signal strength features as well, you can pass the audio signal directly to the AutoSegmenter and get the segments out in units of seconds.\n",
    "\n",
    "The segments returned are a list of 3-tuples, where each tuple specifies that segment's start time, end time, and average signal strength:  `[(seg0_start_seconds, seg0_end_seconds, seg0_strength), (seg1_start_seconds, seg1_end_seconds, seg1_strength), ... ]`.\n",
    "\n",
    "The code below segments the signal and then plots the spectrogram, marking the beginning and end of each detected segment with purple and red lines (respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c077168e-0cba-4b51-b530-4437a38926df",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = AutoSegmenter.get_default_segmenter()\n",
    "segment_time_list = segmenter.segment_signal(audio_signal)\n",
    "segment_start_times = [seg[0] for seg in segment_time_list]\n",
    "segment_end_times = [seg[1] for seg in segment_time_list]\n",
    "\n",
    "print(f\"Number of segments detected = {len(segment_time_list)}\")\n",
    "\n",
    "# --- Visualization code below ---------------------------------------------------------------------\n",
    "\n",
    "# Plot the spectrogram of the file\n",
    "fig = plt.figure(figsize=figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    spectrogram,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        spectrogram_times[0],\n",
    "        spectrogram_times[-1],\n",
    "        spectrogram_frequencies[0],\n",
    "        spectrogram_frequencies[-1],\n",
    "    ],\n",
    ")\n",
    "# Mark the segment start times with purple lines\n",
    "axes.vlines(segment_start_times, 0, 8000, color=\"m\")\n",
    "# Mark the segment end times with red lines\n",
    "axes.vlines(segment_end_times, 0, 8000, color=\"r\")\n",
    "axes.set_xlim([0, audio_signal.duration])\n",
    "axes.set_xlabel(\"Time (seconds)\")\n",
    "axes.set_ylabel(\"Frequency (Hz)\")\n",
    "axes.set_title(\"Spectrogram with detected segments marked\")\n",
    "\n",
    "# Plot the spectrogram of the file\n",
    "fig = plt.figure(figsize=figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    spectrogram,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        spectrogram_times[0],\n",
    "        spectrogram_times[-1],\n",
    "        spectrogram_frequencies[0],\n",
    "        spectrogram_frequencies[-1],\n",
    "    ],\n",
    ")\n",
    "# Mark the segment start times with purple lines\n",
    "axes.vlines(segment_start_times, 0, 8000, color=\"m\")\n",
    "# Mark the segment end times with red lines\n",
    "axes.vlines(segment_end_times, 0, 8000, color=\"r\")\n",
    "axes.set_xlim(fig_xlim)\n",
    "axes.set_xlabel(\"Time (seconds)\")\n",
    "axes.set_ylabel(\"Frequency (Hz)\")\n",
    "axes.set_title(\"Spectrogram with detected segments marked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8bcdcd-f040-4de9-8f00-b84b8c5ac3d1",
   "metadata": {},
   "source": [
    "## Segmenting signal strength features\n",
    "\n",
    "If you do want to use the signal strength features in your processing, it will probably be more convenient to run the segmentation on the signal strength features themselves.  In this case, the start and end of each segment will be returned as indexes along the x-axis of the features array instead of being returned as time values.  This will make it easier to slice out sections of the features corresponding to detected segments since the indexes can be used directly to do the slicing.  Note that the detected segments should be equivalent to the results from running on the signal directly.\n",
    "\n",
    "So in this case, the returned segments will be a list of 3-tuples in the form:   `[(seg0_start_index, seg0_end_index, seg0_strength), (seg1_start_index, seg1_end_index, seg1_strength), ... ]`.\n",
    "\n",
    "The code below demonstrates running segmentation directly on the features, and then plots the results using features sample indexes along the x-axis instead of time.  Then it demonstrates how to slice out the features for one of the segments (randomly selected each time the cell is run) and plots that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3be31b-0764-40f0-8b80-2a3ff303ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run segmentation on the signal strength features\n",
    "segment_index_list = segmenter.segment_signal_strength_features(signal_strength_features)\n",
    "segment_start_indexes = [s[0] for s in segment_index_list]\n",
    "segment_end_indexes = [s[1] for s in segment_index_list]\n",
    "\n",
    "print(f\"Number of segments detected = {len(segment_index_list)}\")\n",
    "\n",
    "# Select a segment to look at\n",
    "segment_index = np.random.randint(len(segment_index_list))   # Randomly pick the segment (most are chirps)\n",
    "#segment_index = 5    # A chirp\n",
    "#segment_index = 92   # Mechanical noise with a chirp at the end\n",
    "#segment_index = 94   # Mechanical noise\n",
    "\n",
    "# Slice out the features for the selected segment\n",
    "segment = segment_index_list[segment_index]\n",
    "segment_features = signal_strength_features.features[:, segment[0]:segment[1]]\n",
    "\n",
    "# --- Visualization code below ---------------------------------------------------------------------\n",
    "\n",
    "# Plot the results, using sample indexes on the x axis instead of time\n",
    "fig = plt.figure(figsize=figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    signal_strength_features.features,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        0,\n",
    "        signal_strength_features.n_samples,\n",
    "        signal_strength_features.frequency_axis[0],\n",
    "        signal_strength_features.frequency_axis[-1],\n",
    "    ],\n",
    ")\n",
    "axes.vlines(segment_start_indexes, 0, 8000, color=\"m\")\n",
    "axes.vlines(segment_end_indexes, 0, 8000, color=\"r\")\n",
    "# Determine the indexes to zoom in on to get the same zoom as above\n",
    "fig_xlim_idx = (sum(signal_strength_features.time_axis < fig_xlim[0]), sum(signal_strength_features.time_axis <= fig_xlim[1]))\n",
    "axes.set_xlabel(\"Sample index\")\n",
    "axes.set_ylabel(\"Frequency (Hz)\")\n",
    "axes.set_title(\"Signal strength features with detected segments marked\")\n",
    "\n",
    "# Plot the results, using sample indexes on the x axis instead of time\n",
    "fig = plt.figure(figsize=figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    signal_strength_features.features,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        0,\n",
    "        signal_strength_features.n_samples,\n",
    "        signal_strength_features.frequency_axis[0],\n",
    "        signal_strength_features.frequency_axis[-1],\n",
    "    ],\n",
    ")\n",
    "axes.vlines(segment_start_indexes, 0, 8000, color=\"m\")\n",
    "axes.vlines(segment_end_indexes, 0, 8000, color=\"r\")\n",
    "# Determine the indexes to zoom in on to get the same zoom as above\n",
    "fig_xlim_idx = (sum(signal_strength_features.time_axis < fig_xlim[0]), sum(signal_strength_features.time_axis <= fig_xlim[1]))\n",
    "axes.set_xlim(fig_xlim_idx)\n",
    "axes.set_xlabel(\"Sample index\")\n",
    "axes.set_ylabel(\"Frequency (Hz)\")\n",
    "axes.set_title(\"Signal strength features with detected segments marked\")\n",
    "\n",
    "# Plot the features for the randomly selected segment\n",
    "fig = plt.figure(figsize=small_figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    segment_features,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        0,\n",
    "        segment_features.shape[1],\n",
    "        signal_strength_features.frequency_axis[0],\n",
    "        signal_strength_features.frequency_axis[-1],\n",
    "    ],\n",
    ")\n",
    "axes.set_xlabel(\"Sample index\")\n",
    "axes.set_ylabel(\"Frequency (Hz)\")\n",
    "axes.set_title(f\"Features for segment {segment_index}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac9f39-5916-4e64-a26b-f171e35be6ad",
   "metadata": {},
   "source": [
    "### Converting segments from sample indexes to time values\n",
    "\n",
    "If you need starting and ending times (in seconds) for the segments in addition to the starting and ending sample indexes, the `AutoSegmenter` class also has a `convert_segments_from_indexes_to_seconds()` function that can be used to do the conversion.  The use of this function is demonstrated below.  Note that the x-axis in the resulting plot is now in units of time (seconds) instead of sample index values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912c6fa-a931-49e9-a78c-a5e9d65c6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_time_list = segmenter.convert_segments_from_indexes_to_seconds(segment_index_list, signal_strength_features)\n",
    "segment_start_times = [s[0] for s in segment_time_list]\n",
    "segment_end_times = [s[1] for s in segment_time_list]\n",
    "\n",
    "# --- Visualization code below ---------------------------------------------------------------------\n",
    "\n",
    "# Plot the features and mark the segments, using units of time on the x-axis\n",
    "fig = plt.figure(figsize=figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    signal_strength_features.features,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        signal_strength_features.time_axis[0],\n",
    "        signal_strength_features.time_axis[-1],\n",
    "        signal_strength_features.frequency_axis[0],\n",
    "        signal_strength_features.frequency_axis[-1],\n",
    "    ],\n",
    ")\n",
    "# Mark the start and end of each segment (using units of time)\n",
    "axes.vlines(segment_start_times, 0, 8000, color=\"m\")\n",
    "axes.vlines(segment_end_times, 0, 8000, color=\"r\")\n",
    "# Zoom in the same as for previous plots\n",
    "axes.set_xlim(fig_xlim)\n",
    "axes.set_xlabel(\"Time (seconds)\")\n",
    "axes.set_ylabel(\"Frequency (Hz)\")\n",
    "axes.set_title(\"Spectrogram\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6a17f-4aed-4a36-979f-180f6cc33e2f",
   "metadata": {},
   "source": [
    "# Pitch tracking\n",
    "\n",
    "The `PitchTracker` class contains a `track_pitch_across_segment()` method for tracking a strong frequency peak (e.g. a chirp sound) across a segment.  Note that if there is not any strong frequency peak present in the segment, it will still generate a track going across the entire segment, trying to fit it to any peaks it finds as best as it can.\n",
    "\n",
    "The `track_pitch_across_segment()` method returns a tuple containing two lists, `(pitch_indexes, peak_strengths)`.  The `pitch_indexes` list contains the index of the detected pitches for each time window in the segment, and the `peak_strengths` list contains numbers representing the relative strength of the detected frequency peak for each time window.  It will generally be larger if there is more energy at the detected pitch, and less energy in the frequency bands a little ways above and below the detected pitch.\n",
    "\n",
    "The code below demonstrates running the pitch tracking on the extracted segment features, and visualizes the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8090e4-68bf-4d30-8af9-bb6f9080a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pitch tracking on features we sliced out for the segment we randomly selected above\n",
    "pitch_tracker = PitchTracker()\n",
    "pitch_indexes, peak_strength = pitch_tracker.track_pitch_across_segment(segment_features)\n",
    "# Convert the pitch indexes to the corresponding frequency values (in Hz)\n",
    "pitch_frequencies = pitch_tracker.pitch_indexes_to_frequencies(pitch_indexes, signal_strength_features.frequency_axis)\n",
    "print(f\"pitch_indexes = {pitch_indexes}\")\n",
    "print(f\"pitch_frequencies = {pitch_frequencies}\")\n",
    "\n",
    "# --- Visualization code below ---------------------------------------------------------------------\n",
    "\n",
    "# Plot the features with the pitch track drawn on top\n",
    "fig = plt.figure(figsize=small_figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    segment_features,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        0,\n",
    "        segment_features.shape[1],\n",
    "        signal_strength_features.frequency_axis[0],\n",
    "        signal_strength_features.frequency_axis[-1],\n",
    "    ],\n",
    ")\n",
    "# Draw a skinny line showing the tracked pitch (add 0.5 to center each point horizontally on the features heatmap)\n",
    "axes.plot(np.arange(len(pitch_frequencies)) + 0.5, pitch_frequencies, color=\"r\", linewidth=1)\n",
    "# Draw a very fat, mostly transparent line to highlight the pitch track / make it more visible\n",
    "axes.plot(np.arange(len(pitch_frequencies)) + 0.5, pitch_frequencies, color=\"r\", linewidth=40, alpha=0.1)\n",
    "axes.set_xlabel(\"Time window index\")\n",
    "axes.set_ylabel(\"Frequency (Hz)\")\n",
    "axes.set_title(f\"Pitch tracked across segment {segment_index}\")\n",
    "\n",
    "# Plot the peak_strength\n",
    "fig = plt.figure(figsize=small_figure_size)\n",
    "axes = fig.add_subplot(1,1,1)\n",
    "# Add 0.5 to the x-coordinates here to to line it up with the above plot better.\n",
    "axes.plot(np.arange(len(pitch_frequencies)) + 0.5, peak_strength)\n",
    "axes.set_xlim((0, len(pitch_frequencies)))\n",
    "axes.set_xlabel(\"Time window index\")\n",
    "axes.set_ylabel(\"Peak strength\")\n",
    "axes.set_title(\"Peak strength\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a412c8-91cb-4b51-906a-b4d753a4b11f",
   "metadata": {},
   "source": [
    "### Compute total upsweep and downsweep of the pitch track\n",
    "\n",
    "I think that the total amounts that the pitch track rises and falls in frequency (Hz) could be useful features in characterizing bird chirp sounds.  I've arbitrarily decided to call these values \"upsweep\" and \"downsweep\", and have provided a simple utility function that can compute them from the list of tracked pitches.  Other values like the minimum pitch, maximum pitch, and median pitch might also be useful features, but can easily be extracted using built-in numpy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb83794-2210-47b2-8399-f4d6cee7e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_upsweep, total_downsweep = compute_pitch_upsweep_downsweep(pitch_frequencies)\n",
    "print(f\"total_upsweep   = {total_upsweep:8.2f} Hz  (total amount the pitch rose over the segment)\")\n",
    "print(f\"total_downsweep = {total_downsweep:8.2f} Hz  (total amount the pitch fell over the segment)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b6595-a4f0-4de3-a92c-9e7a22852c3f",
   "metadata": {},
   "source": [
    "# Segment-level features and dimensionality reduction\n",
    "\n",
    "## Dimensionality reduction along the frequency axis\n",
    "\n",
    "The signal strength features shown above tend to be very high dimensional (in the same way that spectrograms and/or images are high dimensional), and thus might not be suitable for some types of ML algorithms.  For log mel energy features, the dimensionality along the frequency axis is reduced by applying a triangular mel filter bank to the spectrogram values (a matrix multiplication operation).  Since these signal strength features match the same format of a spectrogram, mel filter banks can be applied to them in the same way to reduce the dimensionality of the frequency axis.\n",
    "\n",
    "The `SpectralFeatures` class contains an `apply_mel_filter_bank()` method that can be used to do this, as shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccfc4ae-3541-4039-8f56-37704b23308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 6 # Desired dimensionality along the frequency axis\n",
    "min_frequency = 1000  # Start of frequency range covered by the filter bank\n",
    "max_frequency = 6000  # End of frequency range covered by the filter bank\n",
    "mel_signal_strength_features = signal_strength_features.apply_mel_filter_bank(n_mels, min_frequency, max_frequency)\n",
    "\n",
    "# --- Visualization code below ---------------------------------------------------------------------\n",
    "\n",
    "# Plot the features and mark the segments, using units of time on the x-axis\n",
    "fig = plt.figure(figsize=figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    mel_signal_strength_features.features,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        mel_signal_strength_features.time_axis[0],\n",
    "        mel_signal_strength_features.time_axis[-1],\n",
    "        0,\n",
    "        n_mels,\n",
    "    ],\n",
    ")\n",
    "# Manually set up the y-axis labels since the frequency spacing is non-linear (add 0.5 to center the labels vertically on each row)\n",
    "axes.set_yticks(ticks=np.arange(n_mels) + 0.5)\n",
    "axes.set_yticklabels([f\"{freq:.0f}\" for freq in mel_signal_strength_features.frequency_axis])\n",
    "# Zoom in the same as for previous plots\n",
    "axes.set_xlim(fig_xlim)\n",
    "axes.set_xlabel(\"Time (seconds)\")\n",
    "axes.set_ylabel(\"Center Frequency (Hz)\")\n",
    "axes.set_title(\"Mel signal strength features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93ce41-2012-433f-961f-422ecb4f3561",
   "metadata": {},
   "source": [
    "Another approach could be to take various summary statistics (mean, variance, support, median, max, etc) over the columns of the signal strength features, thus reducing the dimensionality along that axis to the number of statistics taken.  Since those summary statistics are usually dependant on the distribution of values in the column but are invariant to the actual vertical locations of those values, this approach may particularly be attractive for sound types that may shift up or down in frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4489620-9e2e-4628-a888-e8d4cc78d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute some summary statistics along the columns\n",
    "column_statistics = np.vstack((\n",
    "    np.mean(signal_strength_features.features, axis=0),\n",
    "    np.var(signal_strength_features.features, axis=0),\n",
    "    np.max(signal_strength_features.features, axis=0),\n",
    "    np.median(signal_strength_features.features, axis=0),\n",
    "))\n",
    "\n",
    "# --- Visualization code below ---------------------------------------------------------------------\n",
    "\n",
    "# Plot the column statistics\n",
    "fig = plt.figure(figsize=figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    column_statistics ** (1/3),\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        mel_signal_strength_features.time_axis[0],\n",
    "        mel_signal_strength_features.time_axis[-1],\n",
    "        0,\n",
    "        column_statistics.shape[0],\n",
    "    ],\n",
    ")\n",
    "# Manually set up the y-axis labels to label each statistic type (add 0.5 to center the labels vertically on each row)\n",
    "axes.set_yticks(ticks=np.arange(column_statistics.shape[0]) + 0.5)\n",
    "axes.set_yticklabels([\"mean\", \"variance\", \"max\", \"median\"])\n",
    "# Zoom in the same as for previous plots\n",
    "axes.set_xlim(fig_xlim)\n",
    "axes.set_xlabel(\"Time (seconds)\")\n",
    "axes.set_ylabel(\"Column statistics\")\n",
    "axes.set_title(\"Column statistic features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecec05f8-7d10-41b1-85a7-73f8da82df57",
   "metadata": {},
   "source": [
    "## Segment-level features / dimensonality reduction along the time axis\n",
    "\n",
    "Assuming the auto-segmentation works well, a good approach would be to use it as the first step in processing, and then try to classify and/or analyze the types of sound present in each detected segment.  The above code demonstrates how to slice out signal strength features for specific segments.  However, one problem with this is that the sliced out features will have different numbers of dimensions along the time axis depending on how long each detected segment is.  There will be more features (more columns of data) for long segments, and fewer features for short segments.  But most ML algorithms / models require the dimensionality of the input features to be the same for all samples.\n",
    "\n",
    "One approach for dealing with this is to use summary statistics to squash the variable-length time axis into a known number of dimensions.  For instance, you could split each segment up into thirds and then compute the mean values of various features over each third, yielding a consistent number of features regardless of how long the segment is.  The `resize_features_by_averaging()` function from `audiot.spectral_features.py` can be used to do this.  The code below demonstrates this functionality using the features previously sliced out for a single segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33416dd3-ae2a-4973-9628-369de66e38aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break the segment into desired_n_columns time ranges and average the signal strength features over each chunk\n",
    "desired_n_columns = 3\n",
    "resized_segment_features = resize_matrix_by_averaging(segment_features, (segment_features.shape[0], desired_n_columns))\n",
    "\n",
    "# Compute the segment duration in seconds by subtracting the start time from the end time\n",
    "segment_start_time = segment_time_list[segment_index][0]\n",
    "segment_end_time = segment_time_list[segment_index][1]\n",
    "\n",
    "# --- Visualization code below ---------------------------------------------------------------------\n",
    "\n",
    "# Plot the original features for the segment for comparison\n",
    "fig = plt.figure(figsize=small_figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    segment_features,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        segment_start_time,\n",
    "        segment_end_time,\n",
    "        signal_strength_features.frequency_axis[0],\n",
    "        signal_strength_features.frequency_axis[-1],\n",
    "    ],\n",
    ")\n",
    "axes.set_xlabel(\"Time (seconds)\")\n",
    "axes.set_ylabel(\"Frequency (Hz)\")\n",
    "axes.set_title(f\"Original signal strength features for segment {segment_index}\")\n",
    "\n",
    "# Plot the original features for the segment for comparison\n",
    "fig = plt.figure(figsize=small_figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    resized_segment_features,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        segment_start_time,\n",
    "        segment_end_time,\n",
    "        signal_strength_features.frequency_axis[0],\n",
    "        signal_strength_features.frequency_axis[-1],\n",
    "    ],\n",
    ")\n",
    "axes.set_xlabel(\"Time (seconds)\")\n",
    "axes.set_ylabel(\"Frequency (Hz)\")\n",
    "axes.set_title(f\"Signal strength features for segment {segment_index}, resized to have {desired_n_columns} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea8e8a-b5ea-4733-af52-18919f6413d1",
   "metadata": {},
   "source": [
    "The `resize_features_by_averaging()` function from `audiot.spectral_features.py` can also resize along the vertical axis.  This is like using a rectangular filter bank with evenly spaced, non-overlapping rectangular filters (as opposed to a mel filter bank, which uses mel-scale spaced, overlapping triangular filters).  The code below demonstrates this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51492bf-9550-4131-a50d-8d5e506bcbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break the segment into desired_n_rows frequency ranges and desired_n_columns time ranges, averaging the signal strength features over each chunk\n",
    "desired_n_rows = 16\n",
    "resized_segment_features_2 = resize_matrix_by_averaging(segment_features, (desired_n_rows, desired_n_columns))\n",
    "\n",
    "# --- Visualization code below ---------------------------------------------------------------------\n",
    "\n",
    "# Plot the original features for the segment for comparison\n",
    "fig = plt.figure(figsize=small_figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    resized_segment_features_2,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        segment_start_time,\n",
    "        segment_end_time,\n",
    "        signal_strength_features.frequency_axis[0],\n",
    "        signal_strength_features.frequency_axis[-1],\n",
    "    ],\n",
    ")\n",
    "axes.set_xlabel(\"Time (seconds)\")\n",
    "axes.set_ylabel(\"Frequency (Hz)\")\n",
    "axes.set_title(f\"Signal strength features for segment {segment_index}, resized to have {desired_n_rows} rows and {desired_n_columns} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a16e4a-5e04-4905-b47b-55d4df5b5a15",
   "metadata": {},
   "source": [
    "## Extracting features for all segments in a file\n",
    "\n",
    "The `SpectralFeatures` class contains an `extract_grid_features_for_segments()` function that can be used to extract the slices of features for all the segments present in the recording and resize them all to have the same dimensionality (same as above).  This function strings out (`ravel()`) the grid of features for each segment into a column vector, and then stacks the colum vectors side by side to return a matrix containing all the features for all the segments.  The ordering of the `ravel()` operation is such that the feature index cycles through the time windows first, and the frequency windows second.  So with `desired_n_columns=3`, feature indexes 0-2 would be the features for the first, second, and third time windows of the lowest frequency band.  Indexes 3-5 would be the three time windows for the second frequency band, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75757052-d51f-486e-b77e-6def74244789",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_n_rows = 6\n",
    "desired_n_columns = 3\n",
    "\n",
    "# Average each segment's features into a desired_n_rows x desired_n_columns grid and return them as a matrix:\n",
    "features_for_all_segments = signal_strength_features.extract_grid_features_for_segments(segment_index_list, n_freq_divisions=desired_n_rows, n_time_divisions=desired_n_columns)\n",
    "print(f\"features_for_all_segments.shape = {features_for_all_segments.shape}\")\n",
    "\n",
    "# --- Visualization code below ---------------------------------------------------------------------\n",
    "\n",
    "# Plot the features for each segment\n",
    "fig = plt.figure(figsize=small_figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    features_for_all_segments,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        0,\n",
    "        features_for_all_segments.shape[1],\n",
    "        0,\n",
    "        features_for_all_segments.shape[0],\n",
    "    ],\n",
    ")\n",
    "axes.set_xlabel(\"Segment index\")\n",
    "axes.set_ylabel(\"Feature index\")\n",
    "axes.set_title(f\"Features extracted for each of the segments in the entire recording\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453cc9b9-a720-4ba4-9434-b6ae77d6a7d6",
   "metadata": {},
   "source": [
    "If you prefer to use a mel filter bank to reduce dimensionality along the frequency axis, then the `SpectralFeatures.apply_mel_filter_bank()` function can be applied first.  Then you can pass the same number of rows that those features already have (equal to the `n_mels` parameter used in the mel filter bank) as the `n_freq_divsions` parameter so that no additional resizing will be done along the frequency axis.  This is demonstrated below, using the previously computed `mel_signal_strength_features` (which already had `apply_mel_filter_bank()` run on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e63080-d0df-4d46-bf99-d308dfe1f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average each segment's features into a desired_n_rows x desired_n_columns grid and return them as a matrix:\n",
    "mel_features_for_all_segments = mel_signal_strength_features.extract_grid_features_for_segments(segment_index_list, n_freq_divisions=n_mels, n_time_divisions=desired_n_columns)\n",
    "print(f\"mel_features_for_all_segments.shape = {mel_features_for_all_segments.shape}\")\n",
    "\n",
    "# --- Visualization code below ---------------------------------------------------------------------\n",
    "\n",
    "# Plot the features for each segment\n",
    "fig = plt.figure(figsize=small_figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    mel_features_for_all_segments,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        0,\n",
    "        mel_features_for_all_segments.shape[1],\n",
    "        0,\n",
    "        mel_features_for_all_segments.shape[0],\n",
    "    ],\n",
    ")\n",
    "axes.set_xlabel(\"Segment index\")\n",
    "axes.set_ylabel(\"Feature index\")\n",
    "axes.set_title(f\"Features extracted for each of the segments in the entire recording (using a mel filter bank)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e28874-f4ff-428c-aeec-3b6a92e55793",
   "metadata": {},
   "source": [
    "## Computing the chirp features Brandon provided to his teams for chirp detection\n",
    "\n",
    "The features Brandon has provided to his teams (in the form of CSV files) to train chirp detection models on were computed by taking summary statistics over the columns (frequency axis) of the signal strength features, and then averaging those features for each segment over three time windows covering the segment.  The `audiot.signal_processing.functions.py` file contains a `compute_chirp_features_for_segments()` function that computes these same features, and returns them as a Pandas DataFrame.  Note that a `signal_strength` value and the information about each segment (duration, start time, and end time) are also included in the returned DataFrame, as seen below.  The column names have numbers appended to them indicating which time window they came from (e.g. `mean_0` is the mean over the first third of the segment, `mean_1` is the mean over the middle third, and `mean_2` is the mean over the last third).\n",
    "\n",
    "The motivation for using summary statistics along the columns is that they should be relatively invariant to the frequency of the chirps, and thus detectors based on them will hopefully work well even as the pitch of the chirps changes over time due to the chickens growing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29908af0-0f63-47ac-9b3c-8963bf7f95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the same features that Brandon provided to his teams (in CSV files) for chirp detection\n",
    "chirp_features = compute_chirp_features_for_segments(signal_strength_features, segment_index_list)\n",
    "\n",
    "# Display the resulting DataFrame containing the features\n",
    "chirp_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda1ab80-3ae3-42f5-980c-6b16be7b1f17",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "The features extracted for segments can also potentially be passed to clustering algorithms (or other unsupervised algorithms) to try to help separate out the different types of sounds within the data.  The example code below runs clustering on just the segments for this one file, but the same approach can be applied to large numbers of segments extracted from large numbers of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c259f314-b88a-4c12-a727-f48d267aa4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the features previously extracted for all the segments\n",
    "n_clusters = 4\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=2048).fit(features_for_all_segments.T)\n",
    "print(f\"Segment labels from training = {kmeans.labels_}\")\n",
    "print(f\"Re-predicted segment labels  = {kmeans.predict(features_for_all_segments.T)}\")\n",
    "\n",
    "# --- Visualization code below ---------------------------------------------------------------------\n",
    "\n",
    "# Plot the cluster centroids\n",
    "fig = plt.figure(figsize=small_figure_size)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.imshow(\n",
    "    kmeans.cluster_centers_.T,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    "    extent=[\n",
    "        0,\n",
    "        n_clusters,\n",
    "        0,\n",
    "        features_for_all_segments.shape[0],\n",
    "    ],\n",
    ")\n",
    "axes.set_xlabel(\"Cluster index\")\n",
    "axes.set_ylabel(\"Feature index\")\n",
    "axes.set_title(f\"Cluster centroids\")\n",
    "\n",
    "# Plot the features for each cluster\n",
    "for cluster_idx in range(n_clusters):\n",
    "    fig = plt.figure(figsize=small_figure_size)\n",
    "    axes = fig.add_subplot(1, 1, 1)\n",
    "    cluster_features = features_for_all_segments[:,kmeans.labels_ == cluster_idx]\n",
    "    axes.imshow(\n",
    "        cluster_features,\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        interpolation=\"none\",\n",
    "        extent=[\n",
    "            0,\n",
    "            cluster_features.shape[1],\n",
    "            0,\n",
    "            cluster_features.shape[0],\n",
    "        ],\n",
    "    )\n",
    "    axes.set_xlabel(\"Segment index\")\n",
    "    axes.set_ylabel(\"Feature index\")\n",
    "    axes.set_title(f\"Features for each of the segments in cluster {cluster_idx}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicum-tas",
   "language": "python",
   "name": "practicum-tas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
