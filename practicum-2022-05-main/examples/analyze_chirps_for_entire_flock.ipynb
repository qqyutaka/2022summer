{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If running in SageMaker, set up Python environment / install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib.util\n",
    "\n",
    "def is_package_installed(package_name):\n",
    "    if package_name in sys.modules:\n",
    "        return True\n",
    "    else:\n",
    "        return importlib.util.find_spec(package_name) is not None\n",
    "\n",
    "running_on_aws = True if os.environ.get(\"AWS_DEFAULT_REGION\") else False\n",
    "if not running_on_aws:\n",
    "    print(\"Not running on AWS -- Assume that the Python environment is already set up.\")\n",
    "else:\n",
    "    print(\"Running on AWS -- Making sure needed packages are installed...\")\n",
    "    if is_package_installed(\"soundfile\"):\n",
    "        print(\"  Soundfile already installed.\")\n",
    "    else:\n",
    "        print(\"\\n\\nInstalling soundfile...\")\n",
    "        #%pip install pysoundfile\n",
    "        %conda install -c conda-forge pysoundfile\n",
    "    if is_package_installed(\"librosa\"):\n",
    "        print(\"  Librosa already installed.\")\n",
    "    else:\n",
    "        print(\"\\n\\nInstalling librosa (this may take a while)...\")\n",
    "        #%pip install librosa\n",
    "        %conda install -c conda-forge librosa\n",
    "    if is_package_installed(\"audiot\"):\n",
    "        print(\"  AudioT package already installed.\")\n",
    "    else:\n",
    "        print(\"\\n\\nInstalling AudioT package in development / editable mode...\")\n",
    "        # Use the relative path to the folder containing setup.py, which is the parent folder ../ in this case\n",
    "        %pip install -e ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>**If any new packages were installed above, restart the kernel before running the remainder of this notebook**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Python\n",
    "from datetime import datetime, timedelta\n",
    "import io\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import logging\n",
    "import logging.handlers\n",
    "from pathlib import Path\n",
    "\n",
    "# ML / numeric / plotting\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# AWS\n",
    "import boto3\n",
    "\n",
    "# AudioT\n",
    "from audiot.audio_signal import AudioSignal\n",
    "from audiot.dataset import Dataset\n",
    "from audiot.signal_processing.functions import calc_signal_strength_features, compute_pitch_upsweep_downsweep, compute_chirp_features_for_segments\n",
    "from audiot.signal_processing.auto_segmenter import AutoSegmenter\n",
    "from audiot.signal_processing.pitch_tracker import PitchTracker\n",
    "\n",
    "# Show matplotlib plots inline in Jupyter notebooks without having to call show()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure logging\n",
    "\n",
    "This code will likely take a long time to process the full data for a flock (I estimate about 20 hours to process 50 days worth of data).  Your web login to SageMaker will probably time out if you're not present / showing activity during that entire time.  When it times out, the link from stdout to the output visible in this notebook will be broken, so it will look like nothing is happening even though it is actually still running.  To provide a way to still be able to monitor the processing progress, this cell sets up a logger that outputs to both stdout and to a log file.  That way you can monitor the contents of the log file to check if things are still running after logging back in to SageMaker's web interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.handlers.clear()  # Avoid accumulating duplicate handlers in case this cell gets run multiple times\n",
    "log_file = Path(\"analyze_chirps.log\")\n",
    "log_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "file_handler = logging.handlers.RotatingFileHandler(log_file, maxBytes=2**20, backupCount=3)\n",
    "file_handler.setFormatter(log_formatter)\n",
    "logger.addHandler(file_handler)\n",
    "console_handler = logging.StreamHandler(stream=sys.stdout)\n",
    "console_handler.setFormatter(log_formatter)\n",
    "logger.addHandler(console_handler)\n",
    "if log_file.exists():\n",
    "    logger.handlers[0].doRollover()\n",
    "logger.info(\"New log file started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_signal_from_s3(bucket, key):\n",
    "    \"\"\"\n",
    "    Reads an audio file directly from S3 (without saving it to disk) and returns\n",
    "    it as an AudioSignal object.\n",
    "    \n",
    "    Args:\n",
    "        bucket (S3.Bucket): A Bucket object for the bucket to download from.\n",
    "        key (str): The key (or path within the bucket) pointing to the audio file\n",
    "            object to read in.\n",
    "    \"\"\"\n",
    "    buffer = io.BytesIO()\n",
    "    bucket.download_fileobj(key, buffer)\n",
    "    return AudioSignal.from_bytes_io(buffer)\n",
    "\n",
    "def save_results(pickle_file):\n",
    "    \"\"\"\n",
    "    Saves out results so far to the specified pickle file path.\n",
    "    \n",
    "    Note that this function assumes that all the module-level variables below are\n",
    "    defined, and thus will throw errors if it is called before those variables \n",
    "    have actually been defined.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Saving results so far to {pickle_file}.\")\n",
    "    # Put all the variables we want to save into a dictionary\n",
    "    results_dict = {\n",
    "        # Dataset location and information\n",
    "        \"house_number\": house_number,\n",
    "        \"mic_number\": mic_number,\n",
    "        \"bucket_name\": bucket_name,\n",
    "        \"flock_prefix\": flock_prefix,\n",
    "        \"recorder_number\": recorder_number,\n",
    "        \"dataset_prefix\": dataset_prefix,\n",
    "        \"flock_start_date\": flock_start_date,\n",
    "        \"flock_end_date\": flock_end_date,\n",
    "        # Time and frequency axes for results\n",
    "        \"flock_hours\": flock_hours,\n",
    "        \"flock_minutes\": flock_minutes,\n",
    "        \"frequency_axis\": frequency_axis,\n",
    "        # Minute resolution metrics\n",
    "        \"recording_duration_by_minute\": recording_duration_by_minute,\n",
    "        \"segment_count_by_minute\": segment_count_by_minute, \n",
    "        \"total_segment_duration_by_minute\": total_segment_duration_by_minute,\n",
    "        \"chirp_count_by_minute\": chirp_count_by_minute,\n",
    "        \"total_chirp_duration_by_minute\": total_chirp_duration_by_minute,\n",
    "        # Hour resolution metrics\n",
    "        \"chirp_max_frequency_histogram_by_hour\": chirp_max_frequency_histogram_by_hour,\n",
    "        \"chirp_min_frequency_histogram_by_hour\": chirp_min_frequency_histogram_by_hour,\n",
    "        \"chirp_median_frequency_histogram_by_hour\": chirp_median_frequency_histogram_by_hour,\n",
    "        \"chirp_upsweep_histogram_by_hour\": chirp_upsweep_histogram_by_hour,\n",
    "        \"chirp_downsweep_histogram_by_hour\": chirp_downsweep_histogram_by_hour,\n",
    "        \"chirp_frequency_histogram_by_hour\": chirp_frequency_histogram_by_hour,\n",
    "        # Info on where it currently is in the processing\n",
    "        \"hour_index\": hour_index,\n",
    "        # Info on how long it took to process\n",
    "        \"processing_start_time\": processing_start_time,\n",
    "        \"processing_end_time\": processing_end_time,\n",
    "    }\n",
    "    # Save out the dictionary\n",
    "    with open(pickle_file, \"wb\") as file_out:\n",
    "        pickle.dump(results_dict, file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters / dataset info / S3 connection\n",
    "\n",
    "Update the `house_number` and `mic_number` variables below to the data you want to run on.  (If you want to run on data from a different flock, you'll need to update the `bucket_name`, `flock_prefix`, `flock_start_date`, and `flock_end_date` as well.)\n",
    "    \n",
    "The `debugging` variable below is set to `True` for test runs, and will limit the amount of data that actually gets processed.  It should be set to `False` when you're ready to actually process the full flock's data.  Keep in mind that it will likely take a long time (~20 hours for 50 days worth of data) and that the output below will stop working if the web interface times out, even though the code is still running and processing the data.  At that point, you'll need to check the log file if you want to see the output and be able to monitor its progress in processing the data.  Also, you might not be able to use other notebooks while its running since the python kernel will be tied up.  (Maybe you still could use other notebooks if you use different kernels?  I'm not sure on that.)\n",
    "\n",
    "If you need to interrupt / stop the processing at some point (especially after the web interface has timed out), then the stop button above might not work either.  But you can use `Kernel --> Shut Down All Dernels..` from the menu above to kill it (and any other running kernels).  Then you should be able to run things in this and other notebooks again once the kernel has restarted.  The partial results from the data that did get processed should still be available in the output pickle file.  Currently, the code below is not set up to resume processing where it left off though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a flag indicating if we are debugging / testing or not.  When set to true, it limits the amount of data that will be processed.\n",
    "debugging = True\n",
    "max_recordings_to_process_per_hour_if_debugging = 2\n",
    "\n",
    "# Dataset location and information\n",
    "house_number = 3  # Should be 0, 1, 2, or 3 for TRF0 through TRF3\n",
    "mic_number = 0\n",
    "bucket_name = \"audiot-disk03\"\n",
    "flock_prefix = f\"TRF{house_number}_2020-12/\"\n",
    "recorder_number = mic_number // 4\n",
    "dataset_prefix = f\"{flock_prefix}trf{house_number}-recorder-{recorder_number}/\"\n",
    "\n",
    "flock_start_date = datetime(2020, 12, 14)  # The date the birds were placed\n",
    "flock_end_date = datetime(2021, 2, 3)      # The date the birds were caught\n",
    "\n",
    "# Construct a list of each hour of time during the flock. \n",
    "# (Add a day to the end date to go through the end of that day.)\n",
    "flock_hours = pd.date_range(flock_start_date, flock_end_date + timedelta(days=1), freq=\"H\", closed=\"left\")\n",
    "flock_minutes = pd.date_range(flock_start_date, flock_end_date + timedelta(days=1), freq=\"min\", closed=\"left\")\n",
    "if debugging:\n",
    "    # If we're debugging / testing, limit the amount of data to process\n",
    "    flock_hours = flock_hours[500:510]\n",
    "n_hours = len(flock_hours)\n",
    "n_minutes = len(flock_minutes)\n",
    "\n",
    "# Connect to S3\n",
    "s3 = boto3.resource(\"s3\")\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "# Get information about the frequency axis by loading and computing features for the first file in the dataset.\n",
    "# We need to know what the frequency axis looks like so that we can pre-allocate arrays to store histograms with\n",
    "# bins that match up with the frequency axis.\n",
    "# We assume that the frequency axis is the same for all other files (this is true if they all have the same \n",
    "# sampling rate, which they should).\n",
    "first_file = list(bucket.objects.filter(Prefix=f\"{dataset_prefix}\").limit(1))[0]\n",
    "audio_signal = load_audio_signal_from_s3(bucket, first_file.key)\n",
    "signal_strength_features = calc_signal_strength_features(audio_signal)\n",
    "frequency_axis = signal_strength_features.frequency_axis\n",
    "n_frequency_bins = len(frequency_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get signal processing objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = AutoSegmenter.get_default_segmenter()\n",
    "pitch_tracker = PitchTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-allocate variables to store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minute resolution metrics\n",
    "recording_duration_by_minute = np.zeros(n_minutes)\n",
    "segment_count_by_minute = np.zeros(n_minutes, dtype=int)\n",
    "total_segment_duration_by_minute = np.zeros(n_minutes)\n",
    "chirp_count_by_minute = np.zeros(n_minutes, dtype=int)\n",
    "total_chirp_duration_by_minute = np.zeros(n_minutes)\n",
    "\n",
    "# Hour resolution metrics\n",
    "chirp_max_frequency_histogram_by_hour = np.zeros([n_frequency_bins, n_hours], dtype=int)\n",
    "chirp_min_frequency_histogram_by_hour = np.zeros([n_frequency_bins, n_hours], dtype=int)\n",
    "chirp_median_frequency_histogram_by_hour = np.zeros([n_frequency_bins, n_hours], dtype=int)\n",
    "chirp_upsweep_histogram_by_hour = np.zeros([n_frequency_bins, n_hours], dtype=int)\n",
    "chirp_downsweep_histogram_by_hour = np.zeros([n_frequency_bins, n_hours], dtype=int)\n",
    "chirp_frequency_histogram_by_hour = np.zeros([n_frequency_bins, n_hours], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process all the data and save results to disk\n",
    "\n",
    "Currently, the code below does not run an actual chirp detector, but just assigns classifications randomly or calls everything a chirp.  Actual chirp detector code needs to be plugged in to the section marked (replacing the dummy chirp detection code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how often output should occur (both printing/logging progress and saving out partial results)\n",
    "hours_to_process_per_output = 5\n",
    "\n",
    "# Variables to track processing time\n",
    "processing_start_time = datetime.now()\n",
    "processing_end_time = None  # Define this now so that results can be saved out along the way before it has actually finished\n",
    "logger.info(\"Starting processing of audio files.\")\n",
    "\n",
    "# Build a pickle file name to output results to.  Make it unique based on the house/mic numbers and when this was executed to prevent overwriting previous results.\n",
    "timestamp_str = processing_start_time.strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "pickle_file = f\"TRF{house_number}_mic{mic_number:02d}_chirp_results_{timestamp_str}.pickle\"\n",
    "if debugging:\n",
    "    pickle_file = \"DEBUG_\" + pickle_file  # Name outputs from debugging runs differently\n",
    "\n",
    "# === Process the data =======================================================================================\n",
    "for hour_index, hour in enumerate(flock_hours):\n",
    "    date_str = hour.strftime(\"%Y-%m-%d\")\n",
    "    hour_str = hour.strftime(\"%H\")\n",
    "    recordings_for_hour = bucket.objects.filter(Prefix=f\"{dataset_prefix}{date_str}/{hour_str}/TRF{house_number}_mic{mic_number:02d}\")\n",
    "    \n",
    "    # If debugging, limit the number of files to process per hour\n",
    "    if debugging:\n",
    "        recordings_for_hour = list(recordings_for_hour)\n",
    "        if len(recordings_for_hour) > max_recordings_to_process_per_hour_if_debugging:\n",
    "            recordings_for_hour = recordings_for_hour[0:max_recordings_to_process_per_hour_if_debugging]\n",
    "    \n",
    "    # Process the recordings for each hour\n",
    "    for recording in recordings_for_hour:\n",
    "        dataset_tag, file_datetime, mic_number = Dataset.parse_file_path(recording.key)\n",
    "        minute_index = flock_minutes.get_loc(file_datetime)\n",
    "        \n",
    "        # Load the file and compute features\n",
    "        audio_signal = load_audio_signal_from_s3(bucket, recording.key)\n",
    "        recording_duration_by_minute[minute_index] = audio_signal.duration\n",
    "        signal_strength_features = calc_signal_strength_features(audio_signal)\n",
    "        \n",
    "        # Extract segment information\n",
    "        segment_index_list = segmenter.segment_signal_strength_features(signal_strength_features)\n",
    "        if len(segment_index_list) == 0:\n",
    "            continue  # No further processing to be done if no segments detected\n",
    "        segment_time_list = segmenter.convert_segments_from_indexes_to_seconds(segment_index_list, signal_strength_features)\n",
    "        segment_count_by_minute[minute_index] = len(segment_index_list)\n",
    "        total_segment_duration_by_minute[minute_index] = sum([seg_end - seg_start for seg_start, seg_end, _ in segment_time_list])\n",
    "        \n",
    "        # Extract chirp information\n",
    "        chirp_features = compute_chirp_features_for_segments(signal_strength_features, segment_index_list)\n",
    "        \n",
    "        # ============================================================================================================================================\n",
    "        # TODO: The code delimited by the above and below lines doesn't actually do chirp detection.  Instead, it either randomly classifies \n",
    "        # segments as chirps, or classifies them all as chirps (depending on which line is uncommented).  Replace this code with your actual chirp \n",
    "        # detector.  The expected output is just a list of strings (corresponding to the list of segments) that are either \"chirp\" or \"non-chirp\", \n",
    "        # stored in the chirp_classifications variable.\n",
    "        \n",
    "        #chirp_classifications = random.choices([\"chirp\", \"non-chirp\"], k=len(chirp_features))\n",
    "        chirp_classifications = [\"chirp\"] * len(chirp_features)\n",
    "        # ============================================================================================================================================\n",
    "        \n",
    "        # Separate out the info for the segments classified as chirps\n",
    "        chirp_segment_index_list = [segment for segment, classification in zip(segment_index_list, chirp_classifications) if classification == \"chirp\"]\n",
    "        if len(chirp_segment_index_list) == 0:\n",
    "            continue  # No further processing to be done if no chirps detected\n",
    "        chirp_segment_time_list = [segment for segment, classification in zip(segment_time_list, chirp_classifications) if classification == \"chirp\"]\n",
    "        chirp_count_by_minute[minute_index] = len(chirp_segment_index_list)\n",
    "        total_chirp_duration_by_minute = sum([seg_end - seg_start for seg_start, seg_end, _ in chirp_segment_time_list])\n",
    "        \n",
    "        # Iterate through the segments classified as chirps and apply pitch tracking\n",
    "        for segment_start_idx, segment_end_idx, segment_signal_strength in chirp_segment_index_list:\n",
    "            segment_features = signal_strength_features.features[:, segment_start_idx:segment_end_idx]\n",
    "            pitch_indexes, peak_strength = pitch_tracker.track_pitch_across_segment(segment_features)\n",
    "            max_pitch_index = np.max(pitch_indexes)\n",
    "            min_pitch_index = np.min(pitch_indexes)\n",
    "            median_pitch_index = int(np.median(pitch_indexes))\n",
    "            total_upsweep, total_downsweep = compute_pitch_upsweep_downsweep(pitch_indexes)\n",
    "        \n",
    "            # Add this chirp's info into the hour metric tallies\n",
    "            chirp_max_frequency_histogram_by_hour[max_pitch_index, hour_index] += 1\n",
    "            chirp_min_frequency_histogram_by_hour[min_pitch_index, hour_index] += 1\n",
    "            chirp_median_frequency_histogram_by_hour[median_pitch_index, hour_index] += 1\n",
    "            chirp_upsweep_histogram_by_hour[min(total_upsweep, n_frequency_bins-1), hour_index] += 1\n",
    "            chirp_downsweep_histogram_by_hour[min(total_downsweep, n_frequency_bins-1), hour_index] += 1\n",
    "            # Tally up every tracked pitch across the full duration of the chirp and add them to this histogram.\n",
    "            idx_values, idx_counts = np.unique(pitch_indexes, return_counts=True)\n",
    "            chirp_frequency_histogram_by_hour[idx_values, hour_index] += idx_counts\n",
    "            \n",
    "    # Periodically print the progress, estimate completion time, and save results so far.\n",
    "    if hour_index % hours_to_process_per_output == hours_to_process_per_output - 1:\n",
    "        save_results(pickle_file)\n",
    "        current_time = datetime.now()\n",
    "        duration_per_hour = (current_time - processing_start_time) / (hour_index + 1)\n",
    "        estimated_remaining_duration = (n_hours - hour_index - 1) * duration_per_hour\n",
    "        logger.info(f\"Finished processing {hour_index+1} / {n_hours} hours.  Estimated time remaining = {estimated_remaining_duration}.\")\n",
    "\n",
    "# Save out the final results\n",
    "processing_end_time = datetime.now()\n",
    "save_results(pickle_file)\n",
    "logger.info(f\"Finished processing all files.  Total processing duration = {processing_end_time - processing_start_time}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize some of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot minute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_size = [30, 5]\n",
    "\n",
    "plt.figure(figsize=figure_size)\n",
    "plt.plot(flock_minutes, segment_count_by_minute)\n",
    "plt.plot(flock_minutes, chirp_count_by_minute)\n",
    "plt.legend([\"segment_count\", \"chirp_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot hour metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=figure_size)\n",
    "plt.imshow(chirp_frequency_histogram_by_hour, aspect=\"auto\", origin=\"lower\", interpolation=\"none\", extent=[0, n_hours/24, frequency_axis[0], frequency_axis[-1]])\n",
    "plt.title(\"Frequency histograms of all chirps, by hour\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=figure_size)\n",
    "plt.imshow(chirp_max_frequency_histogram_by_hour, aspect=\"auto\", origin=\"lower\", interpolation=\"none\", extent=[0, n_hours/24, frequency_axis[0], frequency_axis[-1]])\n",
    "plt.title(\"Histogram of maximum frequencies of chirps, by hour\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=figure_size)\n",
    "plt.imshow(chirp_min_frequency_histogram_by_hour, aspect=\"auto\", origin=\"lower\", interpolation=\"none\", extent=[0, n_hours/24, frequency_axis[0], frequency_axis[-1]])\n",
    "plt.title(\"Histogram of minimum frequencies of chirps, by hour\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=figure_size)\n",
    "plt.imshow(chirp_median_frequency_histogram_by_hour, aspect=\"auto\", origin=\"lower\", interpolation=\"none\", extent=[0, n_hours/24, frequency_axis[0], frequency_axis[-1]])\n",
    "plt.title(\"Histogram of median frequencies of chirps, by hour\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=figure_size)\n",
    "plt.imshow(chirp_upsweep_histogram_by_hour, aspect=\"auto\", origin=\"lower\", interpolation=\"none\", extent=[0, n_hours/24, frequency_axis[0], frequency_axis[-1]])\n",
    "plt.title(\"Histogram of chirp upsweep, by hour\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.ylim([frequency_axis[0], frequency_axis[-1]/2])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=figure_size)\n",
    "plt.imshow(chirp_downsweep_histogram_by_hour, aspect=\"auto\", origin=\"lower\", interpolation=\"none\", extent=[0, n_hours/24, frequency_axis[0], frequency_axis[-1]])\n",
    "plt.title(\"Histogram of chirp downsweep, by hour\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.ylim([frequency_axis[0], frequency_axis[-1]/2])\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
